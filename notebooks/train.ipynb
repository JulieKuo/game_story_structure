{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root = '/home/coder/projects/test/keyword'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print(f\"{root = }\")\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from src.utils import Log, helper_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed and cudnn\n",
    "helper_function.set_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logger\n",
    "log = Log()\n",
    "logger = log.set_logger(file_path = f\"{root}/logs/log.log\", level = 1, freq = \"D\", interval = 10, backup = 3, name = \"log\")\n",
    "\n",
    "paths = {\n",
    "    'data': f'{root}/data',\n",
    "    'reports': f'{root}/reports',\n",
    "    'models': f'{root}/models',\n",
    "    'src': f'{root}/src',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'init': {\n",
    "            'paths': paths,\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, paths, logger):\n",
    "        self.paths = paths\n",
    "        self.logger = logger\n",
    "    def setting_mlflow(self):\n",
    "        \"\"\"\n",
    "        setting mlflow configuration to record training process\n",
    "        \"\"\"\n",
    "        mlflow.set_tracking_uri(self.exp_config['mlflow']['tracking_uri'])\n",
    "        mlflow.set_experiment(self.exp_config['mlflow']['experiment_name'])\n",
    "        mlflow.start_run(run_name=self.exp_config['mlflow']['run_name'])\n",
    "        mlflow.set_tags(self.exp_config['mlflow']['tags'])\n",
    "        os.environ['LOGNAME'] = self.exp_config['mlflow']['log_name']\n",
    "\n",
    "    def finish_mlflow(self):\n",
    "        \"\"\"\n",
    "        finish mlflow run\n",
    "        \"\"\"\n",
    "        mlflow.end_run()\n",
    "\n",
    "    def log_to_mlflow(self, model, tokenizer, log_model_flag = False):\n",
    "        \"\"\"\n",
    "        log model to mlflow server\n",
    "        \"\"\"\n",
    "        if log_model_flag:\n",
    "            # log model to mlflow server\n",
    "            components = {\n",
    "                \"model\": model,\n",
    "                \"tokenizer\": tokenizer\n",
    "                }\n",
    "            mlflow.transformers.log_model(\n",
    "                transformers_model=components,\n",
    "                artifact_path=\"tagging_model\",\n",
    "            )\n",
    "        else:\n",
    "            # log model path\n",
    "            model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "            mlflow.log_param(\"model_path\", model_path)\n",
    "\n",
    "        # log config file\n",
    "        mlflow.log_artifact(f\"{root}/config/experiment/config.yml\")\n",
    "\n",
    "# class object\n",
    "self = Train(**params[\"init\"], logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlflow': {'tracking_uri': 'http://localhost:5000',\n",
       "  'experiment_name': 'story-structure',\n",
       "  'run_name': 'test',\n",
       "  'log_name': 'julie_kuo',\n",
       "  'device_id': '5',\n",
       "  'tags': {'base_model': 'MediaTek-Research/Breeze-7B-Instruct-v0_1',\n",
       "   'type': 'CAUSAL_LM',\n",
       "   'loss': 'cross_entropy',\n",
       "   'dataset': 'percy_jackson'}},\n",
       " 'data': {'file_path': '/raw/percy_jackson.txt'},\n",
       " 'model': {'name': 'MediaTek-Research/Breeze-7B-Instruct-v0_1',\n",
       "  'description': 'breeze',\n",
       "  'checkpoint': 'checkpoint-300',\n",
       "  'chunk_size': 256,\n",
       "  'early_stopping_patience': 20,\n",
       "  'seed': 42},\n",
       " 'training_args': {'eval_steps': 20,\n",
       "  'save_steps': 100,\n",
       "  'logging_steps': 20,\n",
       "  'max_steps': 5000,\n",
       "  'save_total_limit': 20,\n",
       "  'per_device_train_batch_size': 2,\n",
       "  'per_device_eval_batch_size': 2,\n",
       "  'gradient_accumulation_steps': 8,\n",
       "  'gradient_checkpointing': True,\n",
       "  'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
       "  'warmup_steps': 100,\n",
       "  'learning_rate': 0.0001,\n",
       "  'optim': 'paged_adamw_8bit',\n",
       "  'logging_strategy': 'steps',\n",
       "  'evaluation_strategy': 'steps',\n",
       "  'save_strategy': 'steps',\n",
       "  'load_best_model_at_end': True,\n",
       "  'fp16': True,\n",
       "  'report_to': 'mlflow'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.exp_config = helper_function.load_config(f\"{paths['src']}/config/experiment/config.yml\")\n",
    "self.exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.setting_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model with peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"載入並預處理文本數據\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # 將文本分割成較小的段落，使用較短的長度\n",
    "    self.chunk_size = self.exp_config[\"model\"][\"chunk_size\"]\n",
    "    chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]\n",
    "    \n",
    "    # 創建dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': chunks\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(model_name):\n",
    "    \"\"\"準備小型基礎模型和分詞器，使用新的量化配置\"\"\"\n",
    "    # 設定 4bit 量化配置\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 啟用 4bit 載入，以減少模型大小和記憶體使用量\n",
    "        bnb_4bit_quant_type=\"nf4\",  # 指定量化類型\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # 設定計算數據類型，以加快運算速度\n",
    "        bnb_4bit_use_double_quant=True  # 啟用雙重量化，提高準確性但略增加記憶體用量\n",
    "    )\n",
    "    \n",
    "    # 載入模型和分詞器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,  # 使用新的量化配置\n",
    "        torch_dtype=torch.float16,  # 設定模型的預設數據類型，以優化記憶體使用量\n",
    "        device_map=\"auto\"  # 自動分配模型到可用的設備上（CPU或GPU）\n",
    "    )\n",
    "    \n",
    "    # 配置 LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # LoRA的秩，控制LoRA矩陣的大小，影響參數數量和計算複雜度\n",
    "        lora_alpha=16,  # LoRA放大因子，調整LoRA參數的學習率 (可以先設成 2倍 r)\n",
    "        target_modules=[\n",
    "            \"q_proj\",    # 查詢投影\n",
    "            \"k_proj\",    # 鍵值投影\n",
    "            \"v_proj\",    # 數值投影\n",
    "            \"o_proj\",    # 輸出投影\n",
    "            \"gate_proj\", # 門控投影\n",
    "            \"up_proj\",   # 上投影\n",
    "            \"down_proj\"  # 下投影\n",
    "        ],  # 指定 LoRA 作用的模型部分\n",
    "        lora_dropout=0.05,  # 設定 LoRA 層的 dropout 率，用於防止過擬合\n",
    "        bias=\"none\",  # 關閉偏差項\n",
    "        task_type=\"CAUSAL_LM\"  # 指定任務類型為因果語言模型\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)  # 準備模型以支持 kbit 訓練\n",
    "    model = get_peft_model(model, lora_config)  # 使用 LoRA 配置加速模型訓練    \n",
    "\n",
    "    # move model to available device\n",
    "    available_device = torch.device(\n",
    "        'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(available_device)\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"將文本轉換為token\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True, # 截斷文本以符合模型的最大長度\n",
    "        max_length=self.chunk_size,  # 設定最大長度\n",
    "        padding=\"max_length\" # 填充文本以符合模型的最大長度\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入數據\n",
    "dataset = load_data(f'{self.paths[\"data\"]}/{self.exp_config[\"data\"][\"file_path\"]}')\n",
    "\n",
    "# 分割訓練集和驗證集\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=self.exp_config[\"model\"][\"seed\"], shuffle=True)\n",
    "train_dataset_raw = train_test_split['train']\n",
    "valid_dataset_raw = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備模型和分詞器\n",
    "model, tokenizer = prepare_model_and_tokenizer(self.exp_config[\"model\"][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理數據集\n",
    "train_dataset = train_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # 將文本轉換為token\n",
    "    batched=True, # 一次處理多個樣本\n",
    "    remove_columns=dataset.column_names # 移除原始文本列\n",
    ")\n",
    "\n",
    "# 處理數據集\n",
    "valid_dataset = valid_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # 將文本轉換為token\n",
    "    batched=True, # 一次處理多個樣本\n",
    "    remove_columns=dataset.column_names # 移除原始文本列\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model checkpoint folder\n",
    "model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(model_path, exist_ok=True) # create new model checkpoint\n",
    "\n",
    "# create logging folder\n",
    "logging_dir = f'{self.paths[\"reports\"]}/{self.exp_config[\"model\"][\"name\"]}/{self.exp_config[\"model\"][\"description\"]}/logs'\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(logging_dir, exist_ok=True) # create new model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練參數配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_path, \n",
    "    logging_dir = logging_dir,\n",
    "    device_map={\"\": 0}, # 指定模型放置在GPU (強制所有參數都在 GPU 0 上)\n",
    "    **self.exp_config['training_args']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定數據整理器\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # 是否隨機遮蔽token，關閉以進行因果語言模型訓練，即預測下一個token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建訓練器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(self.exp_config['model']['early_stopping_patience'])]\n",
    "    # compute_metrics=compute_metrics, # 添加計算指標的函數\n",
    "    # loss_fn=custom_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_dict = helper_function.check_device(model, trainer.args)\n",
    "self.logger.info(f\"-- Model is using {'GPU: ' + str(gpu_dict['device_idx']) if gpu_dict['device_idx'] != -1 else 'CPU'}\")\n",
    "self.logger.info(f\"-- training_args is using {gpu_dict['model_gpu_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.finish_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log model, tokenizer and config file to mlflow\n",
    "self.log_to_mlflow(model, tokenizer, log_model_flag = False)\n",
    "self.finish_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_model(model_name):\n",
    "    \"\"\"載入原始模型\"\"\"    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_name, model_description, checkpoint):\n",
    "    \"\"\"載入微調後的模型\"\"\"\n",
    "    adapter_path = f\"{root}/models/{model_name}/{model_description}/{checkpoint}\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 載入基礎模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # 載入 LoRA 權重\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    model = model.merge_and_unload() # 合併 LoRA 權重並釋放記憶體\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512):\n",
    "    \"\"\"生成回應\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你看過波西傑克森這本關於希臘神話的小說嗎?看過的話說明一下故事主軸。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 原始模型回應 ===\")\n",
    "model, tokenizer = load_original_model(self.exp_config[\"model\"][\"name\"])\n",
    "original_response = generate_response(model, tokenizer, question)\n",
    "print(original_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 微調後模型回應 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1777023efdcc4a25bf3ece3f1e9af6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你看過波西傑克森這本關於希臘神話的小說嗎?看過的話說明一下故事主軸。」\n",
      "「波西·傑克遜。」\n",
      "「對，他寫的是關於『十二神諭』。這故事發生在第二次世界大戰的時候，就是一九四○年。當時的奧林匹斯山是一個神秘的地方，因為人類已經忘記了希臘神話。奧林匹斯山的存在在人類的意識裡已經變得模糊了。而波西·傑克遜在美國西部的一個小鎮裡遇到了一些神秘的力量，然後，他回到紐約。」\n",
      "「然後發生了什麼？」\n",
      "「他發現奧林匹斯山正被某個邪惡的力量襲擊，這個力量正想摧毀奧林匹斯山，消滅眾神。」\n",
      "「這個邪惡的力量是誰？」\n",
      "「他就是克里奧斯·羅德斯，克洛諾斯的兒子。」\n",
      "「克洛諾斯的兒子？克洛諾斯是什麼人？」\n",
      "「克洛諾斯是諸神的父親，就是我們所說的海神。克洛諾斯是一個巨大的巨人，他曾試圖擊敗眾神。在一戰中，他被諸神所滅，但克洛諾斯的靈魂並沒有完全消滅。」\n",
      "「你剛才說的這個克里奧斯·羅德斯是克洛諾斯的兒子，那他的父親是……」\n",
      "「就是克洛諾斯，海神的兒子。」\n",
      "「可是克洛諾斯已經死了啊！」\n",
      "「沒錯，但克洛諾斯並沒有死，他只是被封印了。他仍然活在奧林匹斯山。克里奧斯·羅德斯是克洛諾斯的兒子，他繼承了父親的力量。克洛諾斯曾經把奧林匹斯山的存在隱藏了，因此，奧林匹斯山對於人類來說成了一個神秘的地方。」\n",
      "「所以奧林匹斯山現在還存在嗎？」\n",
      "「是的，它依然存在。波西·傑克遜拯救了奧林匹斯山，阻止了克里奧斯·羅德斯的計劃。克里奧斯·羅德斯的計劃是摧毀奧林匹斯山，並且利用他的父親克洛諾斯的力量來征服世界。」\n",
      "「這就是波塞冬的故事嗎？」\n",
      "「不，波塞冬的故事是另外一個故事。波塞冬是海洋之神。他和克洛諾斯\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 微調後模型回應 ===\")\n",
    "model, tokenizer = load_finetuned_model(\n",
    "    self.exp_config[\"model\"][\"name\"], \n",
    "    self.exp_config[\"model\"][\"description\"], \n",
    "    self.exp_config[\"model\"][\"checkpoint\"]\n",
    "    )\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "選項: 1. 離開 2. 說服 3. 戰鬥\n",
      "選擇: 2. 說服\n",
      "\n",
      "產生後續情節\n",
      "\n",
      "我轉身面對斯隆。「我知道你為什麼在這兒。」\n",
      "「你知道嗎？」\n",
      "「你想要的不是我的生命。你已經得到了。你想要的是……」\n",
      "「我想要你的父親的生命。」\n",
      "「你為什麼？」\n",
      "斯隆的表情變得很嚴肅，彷彿在努力讓自己保持平靜。「因為他……他背叛了我。他背叛了所有的神祇。他背叛了克洛諾斯。他背叛了我們所有人。」\n",
      "「所以你打算殺了他。」\n",
      "「是的。」\n",
      "「你知道他並沒有背叛你嗎？」\n",
      "「我必須這麼做。」\n",
      "「你真的相信克洛諾斯嗎？」\n",
      "斯隆的眼睛發出光芒。「是的，波西，我相信他。我相信他會帶領我們打敗奧林匹斯神。」\n",
      "「可你自己也是奧林匹斯神，斯隆。」\n",
      "「你知道那是怎麼回事嗎？」\n",
      "「你已經死過一次。你已經成為克洛諾斯的奴隸了。」\n",
      "斯隆的目光變得銳利而冷漠。「我不知道你怎麼知道的。」\n",
      "「波塞冬告訴我的。」\n",
      "斯隆的臉色變得陰沉下來，彷彿我剛戳到了一個敏感的問題。「波塞冬，他只是個普通的海洋神祇。他什麼都沒有。」\n",
      "「可是他告訴我……」\n",
      "「他只是在胡說八道。我必須去殺死波塞冬，然後克洛諾斯就會征服所有的神祇。」\n",
      "「你知道他不是真的，斯隆。」\n",
      "「波西，我必須去。你只能阻止我，或者……」\n",
      "「或者什麼？」\n",
      "「或者，你可以幫助我。」\n",
      "「幫助你幹什麼？」\n",
      "「去找到我的父親。去告訴他，告訴他克洛諾斯回來了。」\n",
      "「我告訴你，斯隆。克洛諾斯並沒有回來。」\n",
      "斯隆的目光從我的身上移開，朝了遠處。「波西，你應該走開了。」\n",
      "「我不能走，斯隆。你需要我的幫助。」\n",
      "「波西，你已經幫助我了。」\n"
     ]
    }
   ],
   "source": [
    "question = '''\n",
    "選項: 1. 離開 2. 說服 3. 戰鬥\n",
    "選擇: 2. 說服\n",
    "\n",
    "產生後續情節\n",
    "'''\n",
    "\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
