{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root = '/home/juliekuo/projects/story_structure'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print(f\"{root = }\")\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from src.utils import Log, helper_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logger\n",
    "log = Log()\n",
    "logger = log.set_logger(file_path = f\"{root}/logs/log.log\", level = 1, freq = \"D\", interval = 10, backup = 3, name = \"log\")\n",
    "\n",
    "paths = {\n",
    "    'data': f'{root}/data',\n",
    "    'reports': f'{root}/reports',\n",
    "    'models': f'{root}/models',\n",
    "    'src': f'{root}/src',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'init': {\n",
    "            'paths': paths,\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, paths, logger):\n",
    "        self.paths = paths\n",
    "        self.logger = logger\n",
    "    def setting_mlflow(self):\n",
    "        \"\"\"\n",
    "        setting mlflow configuration to record training process\n",
    "        \"\"\"\n",
    "        mlflow.set_tracking_uri(self.exp_config['mlflow']['tracking_uri'])\n",
    "        mlflow.set_experiment(self.exp_config['mlflow']['experiment_name'])\n",
    "        mlflow.start_run(run_name=self.exp_config['mlflow']['run_name'])\n",
    "        mlflow.set_tags(self.exp_config['mlflow']['tags'])\n",
    "        os.environ['LOGNAME'] = self.exp_config['mlflow']['log_name']\n",
    "\n",
    "    def finish_mlflow(self):\n",
    "        \"\"\"\n",
    "        finish mlflow run\n",
    "        \"\"\"\n",
    "        mlflow.end_run()\n",
    "\n",
    "    def log_to_mlflow(self, model, tokenizer, log_model_flag = False):\n",
    "        \"\"\"\n",
    "        log model to mlflow server\n",
    "        \"\"\"\n",
    "        if log_model_flag:\n",
    "            # log model to mlflow server\n",
    "            components = {\n",
    "                \"model\": model,\n",
    "                \"tokenizer\": tokenizer\n",
    "                }\n",
    "            mlflow.transformers.log_model(\n",
    "                transformers_model=components,\n",
    "                artifact_path=\"tagging_model\",\n",
    "            )\n",
    "        else:\n",
    "            # log model path\n",
    "            model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "            mlflow.log_param(\"model_path\", model_path)\n",
    "\n",
    "        # log config file\n",
    "        mlflow.log_artifact(f\"{self.paths['src']}/config/experiment/config.yml\")\n",
    "\n",
    "# class object\n",
    "self = Train(**params[\"init\"], logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlflow': {'tracking_uri': 'http://localhost:5000',\n",
       "  'experiment_name': 'story-structure',\n",
       "  'run_name': 'TinyLlama',\n",
       "  'log_name': 'julie_kuo',\n",
       "  'device_id': '5',\n",
       "  'tags': {'base_model': 'TinyLlama',\n",
       "   'type': 'CAUSAL_LM',\n",
       "   'loss': 'cross_entropy',\n",
       "   'dataset': 'percy_jackson'}},\n",
       " 'data': {'file_path': '/raw/percy_jackson.txt'},\n",
       " 'model': {'name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       "  'description': 'TinyLlama',\n",
       "  'checkpoint': 'checkpoint-300',\n",
       "  'chunk_size': 256,\n",
       "  'early_stopping_patience': 10,\n",
       "  'seed': 42},\n",
       " 'training_args': {'eval_steps': 20,\n",
       "  'save_steps': 100,\n",
       "  'logging_steps': 20,\n",
       "  'max_steps': 20000,\n",
       "  'save_total_limit': 20,\n",
       "  'per_device_train_batch_size': 2,\n",
       "  'per_device_eval_batch_size': 2,\n",
       "  'gradient_accumulation_steps': 8,\n",
       "  'gradient_checkpointing': True,\n",
       "  'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
       "  'warmup_steps': 100,\n",
       "  'learning_rate': 0.0001,\n",
       "  'optim': 'paged_adamw_8bit',\n",
       "  'logging_strategy': 'steps',\n",
       "  'eval_strategy': 'steps',\n",
       "  'save_strategy': 'steps',\n",
       "  'load_best_model_at_end': True,\n",
       "  'fp16': True,\n",
       "  'report_to': 'mlflow'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.exp_config = helper_function.load_config(f\"{paths['src']}/config/experiment/config.yml\")\n",
    "self.exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.setting_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed and cudnn\n",
    "helper_function.set_seed(self.exp_config[\"model\"][\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model with peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"è¼‰å…¥ä¸¦é è™•ç†æ–‡æœ¬æ•¸æ“š\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # å°‡æ–‡æœ¬åˆ†å‰²æˆè¼ƒå°çš„æ®µè½ï¼Œä½¿ç”¨è¼ƒçŸ­çš„é•·åº¦\n",
    "    self.chunk_size = self.exp_config[\"model\"][\"chunk_size\"]\n",
    "    chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]\n",
    "    \n",
    "    # å‰µå»ºdataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': chunks\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(model_name):\n",
    "    \"\"\"æº–å‚™å°å‹åŸºç¤æ¨¡å‹å’Œåˆ†è©å™¨ï¼Œä½¿ç”¨æ–°çš„é‡åŒ–é…ç½®\"\"\"\n",
    "    # è¨­å®š 4bit é‡åŒ–é…ç½®\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # å•Ÿç”¨ 4bit è¼‰å…¥ï¼Œä»¥æ¸›å°‘æ¨¡å‹å¤§å°å’Œè¨˜æ†¶é«”ä½¿ç”¨é‡\n",
    "        bnb_4bit_quant_type=\"nf4\",  # æŒ‡å®šé‡åŒ–é¡å‹\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # è¨­å®šè¨ˆç®—æ•¸æ“šé¡å‹ï¼Œä»¥åŠ å¿«é‹ç®—é€Ÿåº¦\n",
    "        bnb_4bit_use_double_quant=True  # å•Ÿç”¨é›™é‡é‡åŒ–ï¼Œæé«˜æº–ç¢ºæ€§ä½†ç•¥å¢åŠ è¨˜æ†¶é«”ç”¨é‡\n",
    "    )\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,  # ä½¿ç”¨æ–°çš„é‡åŒ–é…ç½®\n",
    "        torch_dtype=torch.float16,  # è¨­å®šæ¨¡å‹çš„é è¨­æ•¸æ“šé¡å‹ï¼Œä»¥å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨é‡\n",
    "        device_map=\"auto\"  # è‡ªå‹•åˆ†é…æ¨¡å‹åˆ°å¯ç”¨çš„è¨­å‚™ä¸Šï¼ˆCPUæˆ–GPUï¼‰\n",
    "    )\n",
    "    \n",
    "    # é…ç½® LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # LoRAçš„ç§©ï¼Œæ§åˆ¶LoRAçŸ©é™£çš„å¤§å°ï¼Œå½±éŸ¿åƒæ•¸æ•¸é‡å’Œè¨ˆç®—è¤‡é›œåº¦\n",
    "        lora_alpha=16,  # LoRAæ”¾å¤§å› å­ï¼Œèª¿æ•´LoRAåƒæ•¸çš„å­¸ç¿’ç‡ (å¯ä»¥å…ˆè¨­æˆ 2å€ r)\n",
    "        target_modules=[\n",
    "            \"q_proj\",    # æŸ¥è©¢æŠ•å½±\n",
    "            \"k_proj\",    # éµå€¼æŠ•å½±\n",
    "            \"v_proj\",    # æ•¸å€¼æŠ•å½±\n",
    "            \"o_proj\",    # è¼¸å‡ºæŠ•å½±\n",
    "            \"gate_proj\", # é–€æ§æŠ•å½±\n",
    "            \"up_proj\",   # ä¸ŠæŠ•å½±\n",
    "            \"down_proj\"  # ä¸‹æŠ•å½±\n",
    "        ],  # æŒ‡å®š LoRA ä½œç”¨çš„æ¨¡å‹éƒ¨åˆ†\n",
    "        lora_dropout=0.05,  # è¨­å®š LoRA å±¤çš„ dropout ç‡ï¼Œç”¨æ–¼é˜²æ­¢éæ“¬åˆ\n",
    "        bias=\"none\",  # é—œé–‰åå·®é …\n",
    "        task_type=\"CAUSAL_LM\"  # æŒ‡å®šä»»å‹™é¡å‹ç‚ºå› æœèªè¨€æ¨¡å‹\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)  # æº–å‚™æ¨¡å‹ä»¥æ”¯æŒ kbit è¨“ç·´\n",
    "    model = get_peft_model(model, lora_config)  # ä½¿ç”¨ LoRA é…ç½®åŠ é€Ÿæ¨¡å‹è¨“ç·´    \n",
    "\n",
    "    # move model to available device\n",
    "    available_device = torch.device(\n",
    "        'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(available_device)\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"å°‡æ–‡æœ¬è½‰æ›ç‚ºtoken\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True, # æˆªæ–·æ–‡æœ¬ä»¥ç¬¦åˆæ¨¡å‹çš„æœ€å¤§é•·åº¦\n",
    "        max_length=self.chunk_size,  # è¨­å®šæœ€å¤§é•·åº¦\n",
    "        padding=\"max_length\" # å¡«å……æ–‡æœ¬ä»¥ç¬¦åˆæ¨¡å‹çš„æœ€å¤§é•·åº¦\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æ•¸æ“š\n",
    "dataset = load_data(f'{self.paths[\"data\"]}/{self.exp_config[\"data\"][\"file_path\"]}')\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=self.exp_config[\"model\"][\"seed\"], shuffle=True)\n",
    "train_dataset_raw = train_test_split['train']\n",
    "valid_dataset_raw = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™æ¨¡å‹å’Œåˆ†è©å™¨\n",
    "model, tokenizer = prepare_model_and_tokenizer(self.exp_config[\"model\"][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee936f52241e4e6f91636b153e153165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2923 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bed007b6ae4574b4beb8f85cde9861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# è™•ç†æ•¸æ“šé›†\n",
    "train_dataset = train_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # å°‡æ–‡æœ¬è½‰æ›ç‚ºtoken\n",
    "    batched=True, # ä¸€æ¬¡è™•ç†å¤šå€‹æ¨£æœ¬\n",
    "    remove_columns=dataset.column_names # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—\n",
    ")\n",
    "\n",
    "# è™•ç†æ•¸æ“šé›†\n",
    "valid_dataset = valid_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # å°‡æ–‡æœ¬è½‰æ›ç‚ºtoken\n",
    "    batched=True, # ä¸€æ¬¡è™•ç†å¤šå€‹æ¨£æœ¬\n",
    "    remove_columns=dataset.column_names # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model checkpoint folder\n",
    "model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(model_path, exist_ok=True) # create new model checkpoint\n",
    "\n",
    "# create logging folder\n",
    "logging_dir = f'{self.paths[\"reports\"]}/{self.exp_config[\"model\"][\"name\"]}/{self.exp_config[\"model\"][\"description\"]}/logs'\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(logging_dir, exist_ok=True) # create new model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´åƒæ•¸é…ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_path, \n",
    "    logging_dir = logging_dir,\n",
    "    **self.exp_config['training_args']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šæ•¸æ“šæ•´ç†å™¨\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # æ˜¯å¦éš¨æ©Ÿé®è”½tokenï¼Œé—œé–‰ä»¥é€²è¡Œå› æœèªè¨€æ¨¡å‹è¨“ç·´ï¼Œå³é æ¸¬ä¸‹ä¸€å€‹token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºè¨“ç·´å™¨\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(self.exp_config['model']['early_stopping_patience'])]\n",
    "    # compute_metrics=compute_metrics, # æ·»åŠ è¨ˆç®—æŒ‡æ¨™çš„å‡½æ•¸\n",
    "    # loss_fn=custom_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 04:17:55 INFO -- Model is using GPU: 0\n",
      "2025-01-01 04:17:55 INFO -- training_args is using cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpu_dict = helper_function.check_device(model, trainer.args)\n",
    "self.logger.info(f\"-- Model is using {'GPU: ' + str(gpu_dict['device_idx']) if gpu_dict['device_idx'] != -1 else 'CPU'}\")\n",
    "self.logger.info(f\"-- training_args is using {gpu_dict['model_gpu_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  920/20000 1:35:25 < 33:03:18, 0.16 it/s, Epoch 5/110]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>19.273100</td>\n",
       "      <td>2.374624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>18.497100</td>\n",
       "      <td>2.256217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>17.642600</td>\n",
       "      <td>2.140254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>16.757400</td>\n",
       "      <td>2.062040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.452400</td>\n",
       "      <td>2.010084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>15.943100</td>\n",
       "      <td>1.972733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>15.634200</td>\n",
       "      <td>1.946516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>15.694500</td>\n",
       "      <td>1.927047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>15.314100</td>\n",
       "      <td>1.911805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>15.230200</td>\n",
       "      <td>1.899343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>14.601400</td>\n",
       "      <td>1.887171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>14.772600</td>\n",
       "      <td>1.876967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>14.671300</td>\n",
       "      <td>1.868508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>14.559800</td>\n",
       "      <td>1.858613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>14.520600</td>\n",
       "      <td>1.852736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>14.498200</td>\n",
       "      <td>1.847758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>14.487500</td>\n",
       "      <td>1.839416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>14.391800</td>\n",
       "      <td>1.833594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>14.104800</td>\n",
       "      <td>1.834984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>13.832900</td>\n",
       "      <td>1.831997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>13.630400</td>\n",
       "      <td>1.829705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>13.840100</td>\n",
       "      <td>1.821760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>13.677200</td>\n",
       "      <td>1.819663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>13.754600</td>\n",
       "      <td>1.816167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.842000</td>\n",
       "      <td>1.812153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>13.635100</td>\n",
       "      <td>1.809041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>13.787600</td>\n",
       "      <td>1.805453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>13.427600</td>\n",
       "      <td>1.811746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>12.896200</td>\n",
       "      <td>1.812341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>13.076300</td>\n",
       "      <td>1.808287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>13.069200</td>\n",
       "      <td>1.806628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>13.310700</td>\n",
       "      <td>1.804063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>13.159600</td>\n",
       "      <td>1.802106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>13.183600</td>\n",
       "      <td>1.795704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>13.201900</td>\n",
       "      <td>1.795776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>13.131700</td>\n",
       "      <td>1.791035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>12.821200</td>\n",
       "      <td>1.814875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>12.382500</td>\n",
       "      <td>1.815229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>12.401200</td>\n",
       "      <td>1.809623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>12.452800</td>\n",
       "      <td>1.806627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>12.542700</td>\n",
       "      <td>1.800806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>12.570700</td>\n",
       "      <td>1.798294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>12.418400</td>\n",
       "      <td>1.799620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>12.643700</td>\n",
       "      <td>1.793906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>12.777000</td>\n",
       "      <td>1.793780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>12.395800</td>\n",
       "      <td>1.828202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at /home/juliekuo/projects/story_structure/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/TinyLlama/checkpoint-720/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=14.150202112612517, metrics={'train_runtime': 5734.3556, 'train_samples_per_second': 55.804, 'train_steps_per_second': 3.488, 'total_flos': 2.3580822994944e+16, 'train_loss': 14.150202112612517, 'epoch': 5.054719562243502})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é–‹å§‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run TinyLlama at: http://localhost:5000/#/experiments/110165045259297196/runs/f97f609de5e042128283f73b9186a831\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/110165045259297196\n"
     ]
    }
   ],
   "source": [
    "# log model, tokenizer and config file to mlflow\n",
    "self.log_to_mlflow(model, tokenizer, log_model_flag = False)\n",
    "self.finish_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_model(model_name):\n",
    "    \"\"\"è¼‰å…¥åŸå§‹æ¨¡å‹\"\"\"    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_name, model_description, checkpoint):\n",
    "    \"\"\"è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹\"\"\"\n",
    "    adapter_path = f\"{root}/models/{model_name}/{model_description}/{checkpoint}\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # è¼‰å…¥åŸºç¤æ¨¡å‹\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # è¼‰å…¥ LoRA æ¬Šé‡\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    model = model.merge_and_unload() # åˆä½µ LoRA æ¬Šé‡ä¸¦é‡‹æ”¾è¨˜æ†¶é«”\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512):\n",
    "    \"\"\"ç”Ÿæˆå›æ‡‰\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä½ çœ‹éæ³¢è¥¿å‚‘å…‹æ£®é€™æœ¬é—œæ–¼å¸Œè‡˜ç¥è©±çš„å°èªªå—?çœ‹éçš„è©±èªªæ˜ä¸€ä¸‹æ•…äº‹ä¸»è»¸ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åŸå§‹æ¨¡å‹å›æ‡‰ ===\n",
      "ä½ çœ‹éæ³¢è¥¿å‚‘å…‹æ£®é€™æœ¬é—œæ–¼å¸Œè‡˜ç¥è©±çš„å°èªªå—?çœ‹éçš„è©±èªªæ˜ä¸€ä¸‹æ•…äº‹ä¸»è»¸ã€‚\n",
      "- é€™æœ¬å°èªªå…©å€‹ç¯‡å‰‡æ˜¯ä¸€èµ·è¬›çš„ï¼Œæ•…äº‹ä¸»è»¸æ˜¯å¸Œè‡˜çš„ç¥è©±å‚³èªªï¼Œä½†æ•…äº‹çš„å…©å€‹ç¯‡å‰‡æ˜¯å¾ä¸åŒçš„è§’åº¦è¬›çš„ï¼Œä¸€å€‹æ˜¯å¸Œè‡˜çš„åœ–æ›¸å‚³èªªï¼Œå¦ä¸€å€‹æ˜¯å¸Œè‡˜çš„ç¥è©±å‚³èªªã€‚\n",
      "- ç¬¬ä¸€å€‹ç¯‡è¬›çš„æ˜¯å¸Œè‡˜çš„ç¥è©±å‚³èªªï¼Œå‚³èªªå°è±¡æ˜¯å¸Œè‡˜çš„ç¥ï¼Œä»–å€‘çš„å­˜åœ¨æ˜¯ç”±ç•¶ä»Šçš„å¸Œè‡˜å°è±¡æ‰€æŒæ¡çš„æ¦‚å¿µæ‰€å‰µé€ ï¼Œåœ¨ä»–å€‘çš„å¿ƒä¸­ï¼Œæ‰€æœ‰çš„æ¦‚å¿µéƒ½æ˜¯ç¥çš„æ¦‚å¿µï¼Œä½†å…©è€…ä¹‹é–“çš„é—œä¿‚æ­·ä¹…ä¸æ–·çš„è®ŠåŒ–ï¼Œæ‰€æœ‰çš„ç¥çš„æ¦‚å¿µï¼Œåœ¨å¸Œè‡˜çš„ç¥è©±å‚³èªªä¸­ï¼Œéƒ½æ²’æœ‰å…©è€…ç›¸ç•¶çš„åŒè³ªã€‚\n",
      "- ç¬¬äºŒå€‹ç¯‡å‰‡æ˜¯å¸Œè‡˜çš„åœ–æ›¸å‚³èªªï¼Œåœ–æ›¸å‚³èªªæ˜¯å¸Œè‡˜çš„ç¥è©±å‚³èªªï¼Œå®ƒå€‘çš„ä¸»è»¸æ˜¯å¸Œè‡˜çš„æ›¸ï¼Œè€Œä¸æ˜¯å¸Œè‡˜çš„ç¥ã€‚å¸Œè‡˜çš„åœ–æ›¸å‚³èªªçš„ä¸»è»¸æ˜¯å¸Œè‡˜çš„æ›¸ï¼Œè€Œä¸æ˜¯å¸Œè‡˜çš„ç¥ã€‚\n",
      "\n",
      "è©²å°ï¿½ï¿½\n"
     ]
    }
   ],
   "source": [
    "print(\"=== åŸå§‹æ¨¡å‹å›æ‡‰ ===\")\n",
    "model, tokenizer = load_original_model(self.exp_config[\"model\"][\"name\"])\n",
    "original_response = generate_response(model, tokenizer, question)\n",
    "print(original_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å¾®èª¿å¾Œæ¨¡å‹å›æ‡‰ ===\n",
      "ä½ çœ‹éæ³¢è¥¿å‚‘å…‹æ£®é€™æœ¬é—œæ–¼å¸Œè‡˜ç¥è©±çš„å°èªªå—?çœ‹éçš„è©±èªªæ˜ä¸€ä¸‹æ•…äº‹ä¸»è»¸ã€‚\n",
      "å¡é‡Œæ™®ç´¢Â·å‚‘å…‹æ£®æ˜¯ä¸€ä½è‘—åçš„è«¸ç¥è€ƒå¯Ÿå®˜ã€‚åœ¨ä»–å€‘çš„å¹¾å€‹æ¼”è®Šä¸­ï¼Œä»–å€‘é›–ç„¶çµ¦æˆ‘å€‘å¾ˆå¤šå•é¡Œï¼Œä½†é›–ç„¶æˆ‘å€‘å¾ˆæŠ±æ€¨ï¼Œä½†å»ä¹Ÿå¾ˆå¿«å¿˜è¨˜äº†ã€‚\n",
      "æ³¢è¥¿å‚‘å…‹æ£®æ˜¯ä¸€ä½è‡­ç³Ÿç³Ÿçš„å…’ç«¥ã€‚ä»–çš„å€’æ˜¯ä¸€é¢æ¼‚äº®ã€‚å°æ–¼ä»–ï¼Œæˆ‘å€‘å¾ˆç†Ÿæ‚‰äº†ã€‚ä»–æ˜¯æˆ‘å€‘æœ€å¾Œä¸€æ¬¡ç™¼ç¾çš„å·¨äººã€‚å°æ–¼ä»–ï¼Œæˆ‘å€‘å¾ˆç†Ÿæ‚‰ã€‚ä»–çš„å€’æ˜¯ä¸€é¢æ¼‚äº®ã€‚\n",
      "æˆ‘å€‘åœ¨çˆ¶è¦ªçš„å¤§å­¸è£¡ï¼Œç•¶æ™‚æ˜¯åœ¨è¨“ç·´å­¸ç”Ÿã€‚å¾ä¸€å€‹åŠå‚·ç—›çš„å°å¥³å­©åˆ°ä¸€å€‹å€’éœ‰çš„è€å¯¦é–ƒçˆçš„å‚¢ä¼™ï¼Œé€™æ˜¯æˆ‘å€‘çš„æ¼”è®Šã€‚\n",
      "æˆ‘å€‘çµ‚æ–¼æ‰¾åˆ°äº†æ³¢è¥¿å‚‘å…‹æ£®ã€‚ä»–çš„çœ¼ç›æ¼†é»‘äº†ï¼Œä½†ä»–çš„è‡‰é«”å»ååˆ†ç©©å›ºã€‚æˆ‘å€‘æ‰¾åˆ°äº†ä»–çš„è€æœ‹å‹ã€‚\n",
      "æˆ‘å€‘è·Ÿåœ¨ä»–å€‘èº«å¾Œçš„è·¯ä¸Šï¼Œå°æ–¼ä»–çš„é€™\n"
     ]
    }
   ],
   "source": [
    "print(\"=== å¾®èª¿å¾Œæ¨¡å‹å›æ‡‰ ===\")\n",
    "model, tokenizer = load_finetuned_model(\n",
    "    self.exp_config[\"model\"][\"name\"], \n",
    "    self.exp_config[\"model\"][\"description\"], \n",
    "    self.exp_config[\"model\"][\"checkpoint\"]\n",
    "    )\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é¸é …: 1. é›¢é–‹ 2. èªªæœ 3. æˆ°é¬¥\n",
      "é¸æ“‡: 2. èªªæœ\n",
      "\n",
      "ç”¢ç”Ÿå¾ŒçºŒæƒ…ç¯€\n",
      "é›–ç„¶è²ç†é›…ç‘æ–¯å°æ–¼é›¢é–‹èªªæœçš„æ¦‚å¿µä¸¦æ²’æœ‰è€ƒæ…®ï¼Œä½†æ˜¯æˆ‘å€‘æ‰¿è«¾èªªæœä»–å€‘ã€‚\n",
      "\n",
      "30åˆ†é˜å¾Œ\n",
      "\n",
      "å°‹æ±‚è€…å€‘ååœ¨æ—é‚Šçš„èˆ¹å¸­ä¸Šï¼Œå¹¾ä¹éƒ½åµäº†ã€‚\n",
      "ç›§å…‹æ²’æœ‰è¾¦æ³•ç‚ºå®ƒå€‘è®“ä¸€åˆ‡éå¥½ã€‚\n",
      "ã€Œè«‹å¹«å¿™æ‰¾å€‹å®‰å…¨çš„è·é›¢å§ï¼ã€ä»–å•ã€‚\n",
      "æˆ‘å€‘è®“ä»–å€‘å¾€å¾Œèµ°ã€‚\n",
      "\n",
      "å®‰å¨œè²çµ²å’Œæ ¼æ´›å¼—è·Ÿè‘—æˆ‘å€‘ã€‚\n",
      "ã€Œæˆ‘å€‘ä¸€å®šä¸èƒ½èµ°è‘—é€™å€‹è·¯å•Šï¼ã€å®‰å¨œè²çµ²èªªã€‚\n",
      "ã€Œä½ å€‘å°±æ˜¯åœ¨å°‹æ±‚è®Šæ›å€‹äººçš„å¸³è™Ÿå—ï¼Ÿã€\n",
      "å®‰å¨œè²çµ²ç™¼å‡ºä¸€è²å’’èªã€‚\n",
      "æ ¼æ´›å¼—å°æˆ‘å€‘èªªï¼šã€Œå˜¿ï¼Œæˆ‘å€‘å…©å€‹éƒ½ç©¿è‘—åŒä¸€å¥—è¡£æœï¼Œå“¼ï¼Œæ²’é—œä¿‚ã€‚ã€\n",
      "å®‰å¨œè²çµ²èªªï¼šã€Œæˆ‘å€‘æ˜¯å°‹æ±‚è€…å—ï¼Ÿã€\n",
      "ã€Œæ˜¯çš„ï¼Œã€æˆ‘èªªï¼Œ\n"
     ]
    }
   ],
   "source": [
    "question = '''\n",
    "é¸é …: 1. é›¢é–‹ 2. èªªæœ 3. æˆ°é¬¥\n",
    "é¸æ“‡: 2. èªªæœ\n",
    "\n",
    "ç”¢ç”Ÿå¾ŒçºŒæƒ…ç¯€\n",
    "'''                           \n",
    "\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
