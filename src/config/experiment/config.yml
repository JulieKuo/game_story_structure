mlflow:
  tracking_uri: "http://localhost:5000" # GPU server 的 MLflow，可替換
  experiment_name: "story-structure"
  run_name: "test"
  log_name: "julie_kuo"
  device_id: "5"
  tags:
    base_model: "MediaTek-Research/Breeze-7B-Instruct-v0_1"
    type: "CAUSAL_LM"
    loss: "cross_entropy"
    dataset: "percy_jackson"

data:
  file_path: "/raw/percy_jackson.txt"

model:
  name: "MediaTek-Research/Breeze-7B-Instruct-v0_1" #'MediaTek-Research/Breeze-7B-Instruct-v0_1', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
  description: breeze
  checkpoint: "checkpoint-300"
  chunk_size: 256
  early_stopping_patience: 20
  seed: 42

training_args:
  eval_steps: 20 # 每多少步進行一次評估
  save_steps: 100 # 每多少步保存一次模型
  logging_steps: 20  # 每多少步記錄一次訓練情況
  # num_train_epochs: 2 # 訓練的最大epoch數 (max_steps 和 num_train_epochs 不能同時設定)
  max_steps: 5000 # 訓練的最大步數 (max_steps 和 num_train_epochs 不能同時設定)
  save_total_limit: 20 # 最多保存的模型數量
  per_device_train_batch_size: 2 # 每個設備上的訓練批次大小
  per_device_eval_batch_size: 2 # 每個設備上的評估批次大小 
  gradient_accumulation_steps: 8 # 梯度累積的步數，在硬件限制下增加有效的批次大小
  gradient_checkpointing: true # 啟用梯度檢查點以節省顯存
  gradient_checkpointing_kwargs: {"use_reentrant": false} # 設置 use_reentrant=false，符合 PyTorch 建議
  # weight_decay: 0.01 # 權重衰減，用於正則化和避免過擬合
  # disable_tqdm: false # 是否禁用tqdm進度條
  # warmup_ratio: 0.1 
  warmup_steps: 100 # 學習率漸進增加的步數，在這些步數內學習率會從0漸增到設定的學習率
  learning_rate: 1.0e-4 # 學習率
  # lr_scheduler_type: "linear" # 逐漸減小學習率，有助於在訓練後期獲得更細微的模型更新
  optim: "paged_adamw_8bit" # 使用的優化器，這裡選用支持低精度的AdamW優化器，以配合16位數的設定
  logging_strategy: "steps"
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true # 是否在訓練結束時載入最佳模型
  # metric_for_best_model: "f1_micro"
  fp16: True  # 是否使用16位浮點數訓練，可以減少記憶體使用並可能加速訓練
  report_to: "mlflow"