{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root = '/home/juliekuo/projects/story_structure'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print(f\"{root = }\")\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from src.utils import Log, helper_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logger\n",
    "log = Log()\n",
    "logger = log.set_logger(file_path = f\"{root}/logs/log.log\", level = 1, freq = \"D\", interval = 10, backup = 3, name = \"log\")\n",
    "\n",
    "paths = {\n",
    "    'data': f'{root}/data',\n",
    "    'reports': f'{root}/reports',\n",
    "    'models': f'{root}/models',\n",
    "    'src': f'{root}/src',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'init': {\n",
    "            'paths': paths,\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, paths, logger):\n",
    "        self.paths = paths\n",
    "        self.logger = logger\n",
    "    def setting_mlflow(self):\n",
    "        \"\"\"\n",
    "        setting mlflow configuration to record training process\n",
    "        \"\"\"\n",
    "        mlflow.set_tracking_uri(self.exp_config['mlflow']['tracking_uri'])\n",
    "        mlflow.set_experiment(self.exp_config['mlflow']['experiment_name'])\n",
    "        mlflow.start_run(run_name=self.exp_config['mlflow']['run_name'])\n",
    "        mlflow.set_tags(self.exp_config['mlflow']['tags'])\n",
    "        os.environ['LOGNAME'] = self.exp_config['mlflow']['log_name']\n",
    "\n",
    "    def finish_mlflow(self):\n",
    "        \"\"\"\n",
    "        finish mlflow run\n",
    "        \"\"\"\n",
    "        mlflow.end_run()\n",
    "\n",
    "    def log_to_mlflow(self, model, tokenizer, log_model_flag = False):\n",
    "        \"\"\"\n",
    "        log model to mlflow server\n",
    "        \"\"\"\n",
    "        if log_model_flag:\n",
    "            # log model to mlflow server\n",
    "            components = {\n",
    "                \"model\": model,\n",
    "                \"tokenizer\": tokenizer\n",
    "                }\n",
    "            mlflow.transformers.log_model(\n",
    "                transformers_model=components,\n",
    "                artifact_path=\"tagging_model\",\n",
    "            )\n",
    "        else:\n",
    "            # log model path\n",
    "            model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "            mlflow.log_param(\"model_path\", model_path)\n",
    "\n",
    "        # log config file\n",
    "        mlflow.log_artifact(f\"{self.paths['src']}/config/experiment/config.yml\")\n",
    "\n",
    "# class object\n",
    "self = Train(**params[\"init\"], logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlflow': {'tracking_uri': 'http://localhost:5000',\n",
       "  'experiment_name': 'story-structure',\n",
       "  'run_name': 'TinyLlama',\n",
       "  'log_name': 'julie_kuo',\n",
       "  'device_id': '5',\n",
       "  'tags': {'base_model': 'TinyLlama',\n",
       "   'type': 'CAUSAL_LM',\n",
       "   'loss': 'cross_entropy',\n",
       "   'dataset': 'percy_jackson'}},\n",
       " 'data': {'file_path': '/raw/percy_jackson.txt'},\n",
       " 'model': {'name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       "  'description': 'TinyLlama',\n",
       "  'checkpoint': 'checkpoint-300',\n",
       "  'chunk_size': 256,\n",
       "  'early_stopping_patience': 10,\n",
       "  'seed': 42},\n",
       " 'training_args': {'eval_steps': 20,\n",
       "  'save_steps': 100,\n",
       "  'logging_steps': 20,\n",
       "  'max_steps': 20000,\n",
       "  'save_total_limit': 20,\n",
       "  'per_device_train_batch_size': 2,\n",
       "  'per_device_eval_batch_size': 2,\n",
       "  'gradient_accumulation_steps': 8,\n",
       "  'gradient_checkpointing': True,\n",
       "  'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
       "  'warmup_steps': 100,\n",
       "  'learning_rate': 0.0001,\n",
       "  'optim': 'paged_adamw_8bit',\n",
       "  'logging_strategy': 'steps',\n",
       "  'eval_strategy': 'steps',\n",
       "  'save_strategy': 'steps',\n",
       "  'load_best_model_at_end': True,\n",
       "  'fp16': True,\n",
       "  'report_to': 'mlflow'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.exp_config = helper_function.load_config(f\"{paths['src']}/config/experiment/config.yml\")\n",
    "self.exp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.setting_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed and cudnn\n",
    "helper_function.set_seed(self.exp_config[\"model\"][\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model with peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"載入並預處理文本數據\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # 將文本分割成較小的段落，使用較短的長度\n",
    "    self.chunk_size = self.exp_config[\"model\"][\"chunk_size\"]\n",
    "    chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]\n",
    "    \n",
    "    # 創建dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': chunks\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(model_name):\n",
    "    \"\"\"準備小型基礎模型和分詞器，使用新的量化配置\"\"\"\n",
    "    # 設定 4bit 量化配置\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 啟用 4bit 載入，以減少模型大小和記憶體使用量\n",
    "        bnb_4bit_quant_type=\"nf4\",  # 指定量化類型\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # 設定計算數據類型，以加快運算速度\n",
    "        bnb_4bit_use_double_quant=True  # 啟用雙重量化，提高準確性但略增加記憶體用量\n",
    "    )\n",
    "    \n",
    "    # 載入模型和分詞器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,  # 使用新的量化配置\n",
    "        torch_dtype=torch.float16,  # 設定模型的預設數據類型，以優化記憶體使用量\n",
    "        device_map=\"auto\"  # 自動分配模型到可用的設備上（CPU或GPU）\n",
    "    )\n",
    "    \n",
    "    # 配置 LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # LoRA的秩，控制LoRA矩陣的大小，影響參數數量和計算複雜度\n",
    "        lora_alpha=16,  # LoRA放大因子，調整LoRA參數的學習率 (可以先設成 2倍 r)\n",
    "        target_modules=[\n",
    "            \"q_proj\",    # 查詢投影\n",
    "            \"k_proj\",    # 鍵值投影\n",
    "            \"v_proj\",    # 數值投影\n",
    "            \"o_proj\",    # 輸出投影\n",
    "            \"gate_proj\", # 門控投影\n",
    "            \"up_proj\",   # 上投影\n",
    "            \"down_proj\"  # 下投影\n",
    "        ],  # 指定 LoRA 作用的模型部分\n",
    "        lora_dropout=0.05,  # 設定 LoRA 層的 dropout 率，用於防止過擬合\n",
    "        bias=\"none\",  # 關閉偏差項\n",
    "        task_type=\"CAUSAL_LM\"  # 指定任務類型為因果語言模型\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)  # 準備模型以支持 kbit 訓練\n",
    "    model = get_peft_model(model, lora_config)  # 使用 LoRA 配置加速模型訓練    \n",
    "\n",
    "    # move model to available device\n",
    "    available_device = torch.device(\n",
    "        'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(available_device)\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"將文本轉換為token\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True, # 截斷文本以符合模型的最大長度\n",
    "        max_length=self.chunk_size,  # 設定最大長度\n",
    "        padding=\"max_length\" # 填充文本以符合模型的最大長度\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入數據\n",
    "dataset = load_data(f'{self.paths[\"data\"]}/{self.exp_config[\"data\"][\"file_path\"]}')\n",
    "\n",
    "# 分割訓練集和驗證集\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=self.exp_config[\"model\"][\"seed\"], shuffle=True)\n",
    "train_dataset_raw = train_test_split['train']\n",
    "valid_dataset_raw = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備模型和分詞器\n",
    "model, tokenizer = prepare_model_and_tokenizer(self.exp_config[\"model\"][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee936f52241e4e6f91636b153e153165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2923 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bed007b6ae4574b4beb8f85cde9861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 處理數據集\n",
    "train_dataset = train_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # 將文本轉換為token\n",
    "    batched=True, # 一次處理多個樣本\n",
    "    remove_columns=dataset.column_names # 移除原始文本列\n",
    ")\n",
    "\n",
    "# 處理數據集\n",
    "valid_dataset = valid_dataset_raw.map(\n",
    "    lambda x: tokenize_function(x, tokenizer), # 將文本轉換為token\n",
    "    batched=True, # 一次處理多個樣本\n",
    "    remove_columns=dataset.column_names # 移除原始文本列\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model checkpoint folder\n",
    "model_path = f\"{self.paths['models']}/{self.exp_config['model']['name']}/{self.exp_config['model']['description']}\"\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(model_path, exist_ok=True) # create new model checkpoint\n",
    "\n",
    "# create logging folder\n",
    "logging_dir = f'{self.paths[\"reports\"]}/{self.exp_config[\"model\"][\"name\"]}/{self.exp_config[\"model\"][\"description\"]}/logs'\n",
    "helper_function.remove_directory(model_path) # remove old model checkpoint\n",
    "os.makedirs(logging_dir, exist_ok=True) # create new model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練參數配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_path, \n",
    "    logging_dir = logging_dir,\n",
    "    **self.exp_config['training_args']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定數據整理器\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # 是否隨機遮蔽token，關閉以進行因果語言模型訓練，即預測下一個token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建訓練器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(self.exp_config['model']['early_stopping_patience'])]\n",
    "    # compute_metrics=compute_metrics, # 添加計算指標的函數\n",
    "    # loss_fn=custom_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 04:17:55 INFO -- Model is using GPU: 0\n",
      "2025-01-01 04:17:55 INFO -- training_args is using cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpu_dict = helper_function.check_device(model, trainer.args)\n",
    "self.logger.info(f\"-- Model is using {'GPU: ' + str(gpu_dict['device_idx']) if gpu_dict['device_idx'] != -1 else 'CPU'}\")\n",
    "self.logger.info(f\"-- training_args is using {gpu_dict['model_gpu_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  920/20000 1:35:25 < 33:03:18, 0.16 it/s, Epoch 5/110]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>19.273100</td>\n",
       "      <td>2.374624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>18.497100</td>\n",
       "      <td>2.256217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>17.642600</td>\n",
       "      <td>2.140254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>16.757400</td>\n",
       "      <td>2.062040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.452400</td>\n",
       "      <td>2.010084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>15.943100</td>\n",
       "      <td>1.972733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>15.634200</td>\n",
       "      <td>1.946516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>15.694500</td>\n",
       "      <td>1.927047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>15.314100</td>\n",
       "      <td>1.911805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>15.230200</td>\n",
       "      <td>1.899343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>14.601400</td>\n",
       "      <td>1.887171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>14.772600</td>\n",
       "      <td>1.876967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>14.671300</td>\n",
       "      <td>1.868508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>14.559800</td>\n",
       "      <td>1.858613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>14.520600</td>\n",
       "      <td>1.852736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>14.498200</td>\n",
       "      <td>1.847758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>14.487500</td>\n",
       "      <td>1.839416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>14.391800</td>\n",
       "      <td>1.833594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>14.104800</td>\n",
       "      <td>1.834984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>13.832900</td>\n",
       "      <td>1.831997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>13.630400</td>\n",
       "      <td>1.829705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>13.840100</td>\n",
       "      <td>1.821760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>13.677200</td>\n",
       "      <td>1.819663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>13.754600</td>\n",
       "      <td>1.816167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.842000</td>\n",
       "      <td>1.812153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>13.635100</td>\n",
       "      <td>1.809041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>13.787600</td>\n",
       "      <td>1.805453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>13.427600</td>\n",
       "      <td>1.811746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>12.896200</td>\n",
       "      <td>1.812341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>13.076300</td>\n",
       "      <td>1.808287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>13.069200</td>\n",
       "      <td>1.806628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>13.310700</td>\n",
       "      <td>1.804063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>13.159600</td>\n",
       "      <td>1.802106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>13.183600</td>\n",
       "      <td>1.795704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>13.201900</td>\n",
       "      <td>1.795776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>13.131700</td>\n",
       "      <td>1.791035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>12.821200</td>\n",
       "      <td>1.814875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>12.382500</td>\n",
       "      <td>1.815229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>12.401200</td>\n",
       "      <td>1.809623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>12.452800</td>\n",
       "      <td>1.806627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>12.542700</td>\n",
       "      <td>1.800806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>12.570700</td>\n",
       "      <td>1.798294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>12.418400</td>\n",
       "      <td>1.799620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>12.643700</td>\n",
       "      <td>1.793906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>12.777000</td>\n",
       "      <td>1.793780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>12.395800</td>\n",
       "      <td>1.828202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at /home/juliekuo/projects/story_structure/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/TinyLlama/checkpoint-720/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=14.150202112612517, metrics={'train_runtime': 5734.3556, 'train_samples_per_second': 55.804, 'train_steps_per_second': 3.488, 'total_flos': 2.3580822994944e+16, 'train_loss': 14.150202112612517, 'epoch': 5.054719562243502})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run TinyLlama at: http://localhost:5000/#/experiments/110165045259297196/runs/f97f609de5e042128283f73b9186a831\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/110165045259297196\n"
     ]
    }
   ],
   "source": [
    "# log model, tokenizer and config file to mlflow\n",
    "self.log_to_mlflow(model, tokenizer, log_model_flag = False)\n",
    "self.finish_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_model(model_name):\n",
    "    \"\"\"載入原始模型\"\"\"    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_name, model_description, checkpoint):\n",
    "    \"\"\"載入微調後的模型\"\"\"\n",
    "    adapter_path = f\"{root}/models/{model_name}/{model_description}/{checkpoint}\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 載入基礎模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # 載入 LoRA 權重\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    model = model.merge_and_unload() # 合併 LoRA 權重並釋放記憶體\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512):\n",
    "    \"\"\"生成回應\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你看過波西傑克森這本關於希臘神話的小說嗎?看過的話說明一下故事主軸。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 原始模型回應 ===\n",
      "你看過波西傑克森這本關於希臘神話的小說嗎?看過的話說明一下故事主軸。\n",
      "- 這本小說兩個篇則是一起講的，故事主軸是希臘的神話傳說，但故事的兩個篇則是從不同的角度講的，一個是希臘的圖書傳說，另一個是希臘的神話傳說。\n",
      "- 第一個篇講的是希臘的神話傳說，傳說對象是希臘的神，他們的存在是由當今的希臘對象所掌握的概念所創造，在他們的心中，所有的概念都是神的概念，但兩者之間的關係歷久不斷的變化，所有的神的概念，在希臘的神話傳說中，都沒有兩者相當的同質。\n",
      "- 第二個篇則是希臘的圖書傳說，圖書傳說是希臘的神話傳說，它們的主軸是希臘的書，而不是希臘的神。希臘的圖書傳說的主軸是希臘的書，而不是希臘的神。\n",
      "\n",
      "該小��\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 原始模型回應 ===\")\n",
    "model, tokenizer = load_original_model(self.exp_config[\"model\"][\"name\"])\n",
    "original_response = generate_response(model, tokenizer, question)\n",
    "print(original_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 微調後模型回應 ===\n",
      "你看過波西傑克森這本關於希臘神話的小說嗎?看過的話說明一下故事主軸。\n",
      "卡里普索·傑克森是一位著名的諸神考察官。在他們的幾個演變中，他們雖然給我們很多問題，但雖然我們很抱怨，但卻也很快忘記了。\n",
      "波西傑克森是一位臭糟糟的兒童。他的倒是一面漂亮。對於他，我們很熟悉了。他是我們最後一次發現的巨人。對於他，我們很熟悉。他的倒是一面漂亮。\n",
      "我們在父親的大學裡，當時是在訓練學生。從一個劍傷痛的小女孩到一個倒霉的老實閃爍的傢伙，這是我們的演變。\n",
      "我們終於找到了波西傑克森。他的眼睛漆黑了，但他的臉體卻十分穩固。我們找到了他的老朋友。\n",
      "我們跟在他們身後的路上，對於他的這\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 微調後模型回應 ===\")\n",
    "model, tokenizer = load_finetuned_model(\n",
    "    self.exp_config[\"model\"][\"name\"], \n",
    "    self.exp_config[\"model\"][\"description\"], \n",
    "    self.exp_config[\"model\"][\"checkpoint\"]\n",
    "    )\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "選項: 1. 離開 2. 說服 3. 戰鬥\n",
      "選擇: 2. 說服\n",
      "\n",
      "產生後續情節\n",
      "雖然貝理雅瑞斯對於離開說服的概念並沒有考慮，但是我們承諾說服他們。\n",
      "\n",
      "30分鐘後\n",
      "\n",
      "尋求者們坐在旁邊的船席上，幾乎都吵了。\n",
      "盧克沒有辦法為它們讓一切過好。\n",
      "「請幫忙找個安全的距離吧！」他問。\n",
      "我們讓他們往後走。\n",
      "\n",
      "安娜貝絲和格洛弗跟著我們。\n",
      "「我們一定不能走著這個路啊！」安娜貝絲說。\n",
      "「你們就是在尋求變換個人的帳號嗎？」\n",
      "安娜貝絲發出一聲咒語。\n",
      "格洛弗對我們說：「嘿，我們兩個都穿著同一套衣服，哼，沒關係。」\n",
      "安娜貝絲說：「我們是尋求者嗎？」\n",
      "「是的，」我說，\n"
     ]
    }
   ],
   "source": [
    "question = '''\n",
    "選項: 1. 離開 2. 說服 3. 戰鬥\n",
    "選擇: 2. 說服\n",
    "\n",
    "產生後續情節\n",
    "'''                           \n",
    "\n",
    "finetuned_response = generate_response(model, tokenizer, question)\n",
    "print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
